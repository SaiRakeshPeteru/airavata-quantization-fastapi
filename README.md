# Quantized Airavata

This repository presents a quantized implementation of the `ai4bharat/Airavata` language model, deployed via a FastAPI backend. The goal is to improve inference efficiency on both CPU and GPU by reducing model size and memory requirements through quantization. The project also includes benchmarking scripts to measure latency and throughput.

---

## Project Overview

- **Model**: ai4bharat/Airavata (7B OpenHathi variant fine-tuned on the IndicInstruct dataset)  
- **Quantization**: 4-bit using `bitsandbytes`  
- **Backend**: FastAPI  
- **Tested on**: Google Colab (T4 GPU) with latency and throughput benchmarks

---
## Google Colab Notebook

All quantization, benchmarking, and FastAPI integration were implemented and tested in a single Google Colab notebook.


The notebook includes:
- Quantization using `bitsandbytes`
- FastAPI setup and endpoint testing
- Latency and throughput benchmarking
- Instruction-prompt inference examples

> **Note:** Enable GPU runtime in Colab via `Runtime > Change runtime type > GPU` before running.

---
## Quantization Details

| Parameter                | Value     |
|--------------------------|-----------|
| Quantization Type        | int4      |
| Quantization Format      | nf4       |
| Use Double Quantization  | True      |
| Compute Data Type        | bfloat16  |
| Storage Data Type        | bfloat16  |

The model was quantized using the `bitsandbytes` 4-bit quantization method with Hugging FaceтАЩs `transformers` integration.

---

## About the Original Model

The base model is a 7B parameter instruction-tuned language model from AI4Bharat, fine-tuned using LoRA on the IndicInstruct dataset. This dataset includes diverse instruction-following corpora such as Anudesh, wikiHow, Flan v2, Dolly, Anthropic-HHH, OpenAssistant v1, and LymSys-Chat.

This model was introduced in the technical report *Airavata: Introducing Hindi Instruction-tuned LLM*.  
You can find the training and evaluation code at the [IndicInstruct GitHub repository](https://github.com/AI4Bharat/IndicInstruct).

---

## Repository Structure

| File/Directory            | Description                                   |
|---------------------------|-----------------------------------------------|
| `app.py`                  | FastAPI backend for model inference           |
| `quantize_and_save.py`    | Script to quantize and store model/tokenizer  |
| `benchmark.py`            | Benchmarking script for latency/throughput    |
| `requirements.txt`        | List of Python dependencies                   |
| `README.md`               | Project documentation                         |
| `Quantized_Airavata/`     | Output directory for quantized model          |

> **Note:** The `Quantized_Airavata/` directory is not included in the repository due to file size constraints. It can be generated locally using `quantize_and_save.py`.

---

## Benchmark Results

Benchmarks were conducted on Google Colab using an NVIDIA T4 GPU, comparing the original FP16 model and its quantized counterparts (4-bit and 8-bit).

| Metric       | Base Model (bfloat16) | Quantized Model (4-bit) | Quantized Model (8-bit) |
|--------------|-------------------|--------------------------|--------------------------|
| Latency      | ~1277.75 ms/req   | ~1817.08 ms ms/req          | ~2010.42 /req          |
| Throughput   | ~0.78 req/sec     | ~0.42 req/sec            | ~0.50 req/sec            |
| Model Size   | ~13.74 GB         | ~3.85 GB                 | ~6.70 GB                 |

The quantized model shows higher latency on Colab's T4 GPU, likely due to lack of INT4 kernel optimization. Performance is expected to improve on newer GPUs like A100/H100.
The quantized model shows a **significant reduction in memory usage** while maintaining acceptable response quality and throughput.

---

## Output Comparison on a Hindi Prompt

To qualitatively evaluate the impact of quantization, we tested both the original and quantized models on a Hindi-language instruction prompt.

**Prompt (Hindi):**  
*тАЬрдореИрдВ рдЕрдкрдиреЗ рд╕рдордп рдкреНрд░рдмрдВрдзрди рдХреМрд╢рд▓ рдХреЛ рдХреИрд╕реЗ рд╕реБрдзрд╛рд░ рд╕рдХрддрд╛ рд╣реВрдБ? рдореБрдЭреЗ рдкрд╛рдВрдЪ рдмрд┐рдВрджреБ рдмрддрд╛рдПрдВредтАЭ*  
(*Translation: How can I improve my time management skills? Give me five points.*)

---

### ЁЯФ╖ FP16 (Original Model) Output:


рдпрд╣рд╛рдБ рдкрд╛рдБрдЪ рдмрд┐рдВрджреБ рджрд┐рдП рдЧрдП рд╣реИрдВ рдЬреЛ рдЖрдкрдХреЛ рдЕрдкрдиреЗ рд╕рдордп рдкреНрд░рдмрдВрдзрди рдХреМрд╢рд▓ рдореЗрдВ рд╕реБрдзрд╛рд░ рдХрд░рдиреЗ рдореЗрдВ рдорджрдж рдХрд░ рд╕рдХрддреЗ рд╣реИрдВрдГ

1.рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рджреЗрдВрдГ рдЙрди рдХрд╛рд░реНрдпреЛрдВ рдХреЛ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рджреЗрдВ рдЬрд┐рдиреНрд╣реЗрдВ рдкреВрд░рд╛ рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реИ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рджреЗрдВред

2.рдПрдХ рдЕрдиреБрд╕реВрдЪреА рдмрдирд╛рдПрдБрдГ рдПрдХ рдЕрдиреБрд╕реВрдЪреА рдмрдирд╛рдПрдБ рдФрд░ рдЙрд╕ рдкрд░ рдЯрд┐рдХреЗ рд░рд╣реЗрдВред рдпрд╣ рдЖрдкрдХреЛ рдЕрдкрдиреЗ рдХрд╛рд░реНрдпреЛрдВ рдкрд░ рдзреНрдпрд╛рди рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рдиреЗ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдкреВрд░рд╛ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЖрд╡рд╢реНрдпрдХ рд╕рдордп рджреЗрдиреЗ рдореЗрдВ рдорджрдж рдХрд░реЗрдЧрд╛ред

3.рдмреНрд░реЗрдХ рд▓реЗрдВрдГ рдирд┐рдпрдорд┐рдд рдмреНрд░реЗрдХ рд▓реЗрдВ рдФрд░ рд░рд┐рдЪрд╛рд░реНрдЬ рдХрд░реЗрдВред рдпрд╣ рдЖрдкрдХреЛ рдЕрдзрд┐рдХ рдЙрддреНрдкрд╛рджрдХ рдФрд░ рдХреЗрдВрджреНрд░рд┐рдд рд░рд╣рдиреЗ рдореЗрдВ рдорджрдж рдХрд░реЗрдЧрд╛ред

4.рдзреНрдпрд╛рди рднрдЯрдХрд╛рдиреЗ рд╕реЗ рдмрдЪреЗрдВрдГ рдЕрдкрдиреЗ рдлреЛрди рдХреЛ рдмрдВрдж рдХрд░ рджреЗрдВ рдФрд░ рдзреНрдпрд╛рди рднрдЯрдХрд╛рдиреЗ рд╕реЗ рдмрдЪреЗрдВред рдЗрд╕рд╕реЗ рдЖрдкрдХреЛ рдЕрдкрдиреЗ рдХрд╛рд░реНрдпреЛрдВ рдкрд░ рдзреНрдпрд╛рди рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рдиреЗ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдкреВрд░рд╛ рдХрд░рдиреЗ рдореЗрдВ рдорджрдж рдорд┐рд▓реЗрдЧреАред

5.рд╡реНрдпрд╡рд╕реНрдерд┐рдд рд░рд╣реЗрдВрдГ рдЕрдкрдиреЗ рдХрд╛рд░реНрдпрд╕реНрдерд▓ рдХреЛ рд╡реНрдпрд╡рд╕реНрдерд┐рдд рд░рдЦреЗрдВ рдФрд░ рдЕрдкрдиреЗ рдХрд╛рдЧрдЬрд╛рдд рдФрд░ рдЕрдиреНрдп рд╡рд╕реНрддреБрдУрдВ рдХреЛ рд╡реНрдпрд╡рд╕реНрдерд┐рдд рд░рдЦреЗрдВред рдЗрд╕рд╕реЗ рдЖрдкрдХреЛ рдЕрдкрдиреЗ рдХрд╛рд░реНрдпреЛрдВ рдкрд░ рдзреНрдпрд╛рди рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рдиреЗ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдкреВрд░рд╛ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЖрд╡рд╢реНрдпрдХ рд╕рдордп рджреЗрдиреЗ рдореЗрдВ рдорджрдж рдорд┐рд▓реЗрдЧреАред

---

###  Quantized 4-bit Model Output:
рд╕рдордп рдкреНрд░рдмрдВрдзрди рдХреМрд╢рд▓

1.рдПрдХ рдЕрдиреБрд╕реВрдЪреА рдмрдирд╛рдПрдБрдГ рдПрдХ рдЕрдиреБрд╕реВрдЪреА рдмрдирд╛рдПрдБ рдФрд░ рдЙрд╕ рдкрд░ рдЯрд┐рдХреЗ рд░рд╣реЗрдВред

2.рд╕рдордп рдХрд╛ рдкреНрд░рдмрдВрдзрди рдХрд░реЗрдВрдГ рдХрд╛рд░реНрдпреЛрдВ рдХреЛ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рджреЗрдВ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдкреВрд░рд╛ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдирд┐рд░реНрдзрд╛рд░рд┐рдд рд╕рдордп рдкрд░ рдирд┐рд░реНрдзрд╛рд░рд┐рдд рдХрд░реЗрдВред

3.рдХрд╛рд░реНрдпреЛрдВ рдХреЛ рдкреВрд░рд╛ рдХрд░реЗрдВрдГ рдПрдХ рдмрд╛рд░ рдЬрдм рдЖрдк рдПрдХ рдХрд╛рд░реНрдп рдирд┐рд░реНрдзрд╛рд░рд┐рдд рдХрд░ рд▓реЗрддреЗ рд╣реИрдВ, рддреЛ рдЙрд╕реЗ рдкреВрд░рд╛ рдХрд░реЗрдВред

4.рдзреНрдпрд╛рди рджреЗрдВрдГ рдПрдХ рдХрд╛рд░реНрдп рдкрд░ рдзреНрдпрд╛рди рджреЗрдВ рдФрд░ рдЙрд╕реЗ рдкреВрд░рд╛ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╕рдорд░реНрдкрд┐рдд рд╕рдордп рджреЗрдВред

5.рдмреНрд░реЗрдХ рд▓реЗрдВрдГ рдмреНрд░реЗрдХ рд▓реЗрдирд╛ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реИ рддрд╛рдХрд┐ рдЖрдк рдлрд┐рд░ рд╕реЗ рдКрд░реНрдЬрд╛рд╡рд╛рди рдФрд░ рдЙрддреНрдкрд╛рджрдХ рдорд╣рд╕реВрд╕ рдХрд░ рд╕рдХреЗрдВред"

---


### Observations

- The **original FP16 model** provides more detailed, fluent, and well-formatted output.
- The **quantized model** retains core instructional quality, though it exhibits some redundancy and slightly less coherent phrasing.
- Overall, the quantized model is still effective for Hindi instruction tasks while offering considerable efficiency gains.

---



