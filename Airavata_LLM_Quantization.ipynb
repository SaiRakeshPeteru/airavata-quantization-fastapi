{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries"
      ],
      "metadata": {
        "id": "VPZta-ES6-v3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzOQPmegmTQK"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install bitsandbytes accelerate\n",
        "!pip install -q fastapi uvicorn nest-asyncio pyngrok transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantize the Airavata LLM and save it"
      ],
      "metadata": {
        "id": "4Qq0NsNZ7DuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,        # or load_in_8bit=True for 8-bit quant\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # or float16 depending on your GPU\n",
        ")\n",
        "\n",
        "# Load tokenizer and quantized model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/Airavata\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"ai4bharat/Airavata\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Save both the tokenizer and the quantized model\n",
        "save_dir = \"./Quantized_Airavata\"\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(save_dir)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_dir)\n"
      ],
      "metadata": {
        "id": "k2SP_xtx7OJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create app.py file"
      ],
      "metadata": {
        "id": "VBlaxfeG7p7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "model_path = \"/content/Quantized_Airavata\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Root endpoint (fixes \"Not Found\" at /)\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"âœ… Airavata FastAPI server is running. Use /docs to try it.\"}\n",
        "\n",
        "# Input model\n",
        "class GenerateRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_new_tokens: int = 100\n",
        "\n",
        "# POST endpoint for text generation\n",
        "@app.post(\"/generate\")\n",
        "async def generate(req: GenerateRequest):\n",
        "    output = pipe(\n",
        "        req.prompt,\n",
        "        max_new_tokens=req.max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return {\"generated_text\": output[0][\"generated_text\"]}\n"
      ],
      "metadata": {
        "id": "ahwWnxF47dy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below cell replace your_authtoken_here with your ngrok authtoken"
      ],
      "metadata": {
        "id": "AOyUqn8u8Dmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken your_authtoken_here\n"
      ],
      "metadata": {
        "id": "aNxfwdMZ75GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"ðŸ”— Public URL:\", public_url)\n",
        "\n",
        "# Start server in a thread\n",
        "def run():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "thread = threading.Thread(target=run)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "v2NhiepJ8VSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert the above printed url below. add '/generate' at the end of url"
      ],
      "metadata": {
        "id": "Eg9YQpUt8hvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "url = \"https://123456789.ngrok-free.app/generate\"\n",
        "data = {\n",
        "    \"prompt\": \"how to manage time effectively ?\",\n",
        "    \"max_new_tokens\": 20\n",
        "}\n",
        "\n",
        "n_requests = 20\n",
        "times = []\n",
        "\n",
        "# Warm-up\n",
        "requests.post(url, json=data)\n",
        "\n",
        "for _ in range(n_requests):\n",
        "    start = time.time()\n",
        "    _ = requests.post(url, json=data)\n",
        "    times.append(time.time() - start)\n",
        "\n",
        "avg_latency = sum(times) / n_requests\n",
        "throughput = n_requests / sum(times)\n",
        "\n",
        "print(f\"Avg Latency: {avg_latency * 1000:.2f} ms\")\n",
        "print(f\"Throughput: {throughput:.2f} requests/sec\")\n"
      ],
      "metadata": {
        "id": "_Qyyk9mu80EV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
